# -*- coding: utf-8 -*-
"""Submission_NLP_Dicoding.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LB9zOUJYbDoxxp4qTH1-ln8DhZBY-OZ7
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
#Submission Neural Language Processing
#Muhammad Iodine Hanifan Firdaus

# %cd /content/drive/MyDrive/Dicoding_Machine_Learning_2/Submission1

import pandas as pd
df = pd.read_csv('Emotion_final.csv')
df = df[['Text', 'Emotion']]
df['Text'] = df['Text'].str.replace('/','')
df['Text'] = df['Text'].str.replace('â€™','')
df['Text'] = df['Text'].str.replace('nn','')
df['Text'] = df['Text'].str.replace('    ','')
df['Text'] = df['Text'].str.replace('\'','')
df['Text'] = df['Text'].str.replace('(','')
df['Text'] = df['Text'].str.replace(')','')
df['Text'] = df['Text'].str.replace(':','')
df['Text'] = df['Text'].str.replace('\r',' ')
df['Text'] = df['Text'].str.replace('\\','')
df['Text'] = df['Text'].str.replace('Ã','')
df['Text'] = df['Text'].str.replace('*','')
df['Text'] = df['Text'].str.replace('©','')
df['Text'] = df['Text'].str.replace('Â','')
df.head()

category = pd.get_dummies(df.Emotion)
df_baru = pd.concat([df, category], axis=1)
df_baru = df_baru.drop(columns='Emotion')
df_baru

len(df_baru)

df_baru['Text'][100]

tweet = df_baru['Text'].values
label = df_baru[['anger', 'fear', 'happy', 'love', 'sadness', 'surprise']].values

from sklearn.model_selection import train_test_split
tweet_train, tweet_test, label_train, label_test = train_test_split(tweet, label, test_size=0.2, random_state=32)

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import numpy as np

tokenizer = Tokenizer(num_words=1000, oov_token='x', lower=False)
tokenizer.fit_on_texts(tweet_train)
tokenizer.fit_on_texts(tweet_test)

sequence_train = tokenizer.texts_to_sequences(tweet_train)
sequence_test = tokenizer.texts_to_sequences(tweet_test)

padded_train = pad_sequences(sequence_train)
padded_test = pad_sequences(sequence_test)

import tensorflow as tf
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=1000, output_dim=16),
    tf.keras.layers.LSTM(512, return_sequences=True),
    tf.keras.layers.LSTM(512),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(1024, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(1024, activation='relu'),
    tf.keras.layers.Dense(512, activation='relu'),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(6, activation='softmax')
])
model.summary()
model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])

class myCallbacks(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.9 and logs.get('val_accuracy')>0.9):
      print('Akurasi Tercapai')
      self.model.stop_training = True
callback = myCallbacks()

history = model.fit(
    padded_train, 
    label_train, 
    epochs=50, 
    validation_data=(padded_test, label_test), 
    verbose=2,
    batch_size=256,
    callbacks=[callback]
    )

import matplotlib.pyplot as plt
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Akurasi Model')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Loss Model')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()